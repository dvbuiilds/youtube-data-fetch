{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install python-dotenv\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "!pip install python-decouple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from decouple import config\n",
    "import json\n",
    "from csv import writer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procedure to load keys from \"credentials.json\" and authenticate with Youtube API server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"\"\n",
    "with open (\"credentials.json\", \"r\") as creds:\n",
    "    data = json.load(creds)\n",
    "    KEY = data[\"KEY2\"]\n",
    "\n",
    "videoId1 = \"cljQIHfdSrY\" # palki sharma video 1\n",
    "videoId2 = \"JwHCOI8V4LQ\" # palki sharma video 2\n",
    "videoId3 = \"OcSUWAYBPZs\" # video with less comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version =\"v3\"\n",
    "youtube = build(api_service_name, api_version, developerKey = KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFileName = \"RavishYoutubeSearch2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to append a row in CSV file of the journalist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeInCSV(record):\n",
    "    with open(dataFileName, \"a+\", encoding=\"utf-8\") as dataFile:\n",
    "        csv_writer = writer(dataFile)\n",
    "        csv_writer.writerow(record)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headers for the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headingRow = [ \n",
    "    \"CommentID\", \n",
    "    \"Timestamp\", \n",
    "    \"Comment\", \n",
    "    \"ReplyCount\", \n",
    "    \"LikeCount\", \n",
    "#     \"AuthorChannelId\", \n",
    "#     \"AuthorName\", \n",
    "    \"ParentID\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling the function to write the headers into the file.\n",
    "\n",
    "writeInCSV(headingRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Fetch Comment Threads Object using \"video Id\" of a Youtube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsFromVideoID(videoId):\n",
    "    commentsPerPage = 1000\n",
    "    request = youtube.commentThreads().list(\n",
    "        part = \"snippet,replies\",\n",
    "        maxResults = commentsPerPage,\n",
    "        videoId = videoId,\n",
    "        textFormat = \"plainText\",\n",
    "    )\n",
    "    response = request.execute()\n",
    "    count = 0\n",
    "    \n",
    "    while(response):\n",
    "        count += len(response['items'])\n",
    "        for commentObj in response['items']:\n",
    "            commentId = commentObj['snippet']['topLevelComment']['id']\n",
    "            comment = commentObj['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            replyCount = commentObj['snippet']['totalReplyCount']\n",
    "            likesCount = commentObj['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            timeStamp = commentObj['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            childOf = \"\"\n",
    "            \n",
    "            writeInCSV([\n",
    "                commentId, \n",
    "                timeStamp, \n",
    "                comment, \n",
    "                replyCount, \n",
    "                likesCount, \n",
    "                childOf\n",
    "            ])\n",
    "            \n",
    "            if 'replies' in commentObj:\n",
    "                count += len(commentObj['replies']['comments'])\n",
    "                for i in range(len(commentObj['replies']['comments'])):\n",
    "                    commentId = commentObj['replies']['comments'][i][\"id\"].split('.')[1]\n",
    "                    comment = commentObj['replies']['comments'][i]['snippet']['textDisplay']\n",
    "                    childReplyCount = 0\n",
    "                    likesCount = commentObj['replies']['comments'][i]['snippet']['likeCount']\n",
    "                    timeStamp = commentObj['replies']['comments'][i]['snippet']['publishedAt']\n",
    "                    childOf = commentObj['replies']['comments'][i]['snippet']['parentId']\n",
    "\n",
    "                    writeInCSV([\n",
    "                        commentId, \n",
    "                        timeStamp, \n",
    "                        comment, \n",
    "                        childReplyCount, \n",
    "                        likesCount, \n",
    "                        childOf\n",
    "                    ])\n",
    "    \n",
    "        if 'nextPageToken' in response:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part = 'snippet,replies',\n",
    "                videoId = videoId,\n",
    "                maxResults= commentsPerPage,\n",
    "                pageToken = response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsFromVideoID(videoId):\n",
    "    commentsPerPage = 1000\n",
    "    request = youtube.commentThreads().list(\n",
    "        part = \"snippet,replies\",\n",
    "        maxResults = commentsPerPage,\n",
    "        videoId = videoId,\n",
    "        textFormat = \"plainText\",\n",
    "    )\n",
    "    response = request.execute()\n",
    "    count = 0\n",
    "    \n",
    "    while(response):\n",
    "        count += len(response['items'])\n",
    "        for commentObj in response['items']:\n",
    "            commentId = commentObj['snippet']['topLevelComment']['id']\n",
    "            comment = commentObj['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            replyCount = commentObj['snippet']['totalReplyCount']\n",
    "            likesCount = commentObj['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            timeStamp = commentObj['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            childOf = \"\"\n",
    "            \n",
    "            writeInCSV([commentId, timeStamp, comment, replyCount, likesCount, childOf])\n",
    "            \n",
    "            if 'replies' in commentObj:\n",
    "                count += len(commentObj['replies']['comments'])\n",
    "                for i in range(len(commentObj['replies']['comments'])):\n",
    "                    commentId = commentObj['replies']['comments'][i][\"id\"].split('.')[1]\n",
    "                    comment = commentObj['replies']['comments'][i]['snippet']['textDisplay']\n",
    "                    childReplyCount = 0\n",
    "                    likesCount = commentObj['replies']['comments'][i]['snippet']['likeCount']\n",
    "                    timeStamp = commentObj['replies']['comments'][i]['snippet']['publishedAt']\n",
    "                    childOf = commentObj['replies']['comments'][i]['snippet']['parentId']\n",
    "\n",
    "                    writeInCSV([commentId, timeStamp, comment, childReplyCount, likesCount, childOf])\n",
    "    \n",
    "        if 'nextPageToken' in response:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part = 'snippet,replies',\n",
    "                videoId = videoId,\n",
    "                maxResults= commentsPerPage,\n",
    "                pageToken = response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'queries' holds the list of queries to be searched on Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"palki sharma upadhyay interview\", \n",
    "    \"ravish kumar biography journey\", \n",
    "    \"arnab goswami interview life journey\", \n",
    "    \"anjana om kashyap journey biography\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit on the maximum number of comments to be extracted via API and procedures above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCommentCount = 36000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dataset manually\n",
    "Each of the below cells has to be run again and again for each of the queries declared above as the connection between the system and Youtube is breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making the search call to the API to bring the videos that appear on SERP.\n",
    "\n",
    "request = youtube.search().list(\n",
    "    part=\"snippet\",\n",
    "    maxResults = 100,\n",
    "    q=\"ravish kumar biography journey\",\n",
    "    order=\"relevance\",\n",
    "#     regionCode=\"IND\"\n",
    ")\n",
    "response = request.execute()\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Storing the Search Call object inside a json file.\n",
    "\n",
    "with open (\"responseRavish.json\", \"w\") as fout:\n",
    "    jsonObj = json.dump(response, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the json file and extracting out the 'video id's from the object.\n",
    "\n",
    "with open(\"responseRavish.json\", \"r\") as fread:\n",
    "    response = json.load(fread)\n",
    "\n",
    "videosIDS = list()\n",
    "for i in range(len(response[\"items\"])):\n",
    "    videosIDS.append(response[\"items\"][i][\"id\"][\"videoId\"])\n",
    "\n",
    "### open a file for jouralist.\n",
    "\n",
    "dataFileName = \"\".join(queries[1].split()[:2])+\".csv\"\n",
    "writeInCSV(headingRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Declaring the variables as 0 that will control the loop later.\n",
    "\n",
    "vid = 0\n",
    "commentCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The while loop that fetches the comments from the videos.\n",
    "\n",
    "while(vid < len(videosIDS) and commentCount < maxCommentCount):\n",
    "    commentCount += getCommentsFromVideoID(videosIDS[vid])\n",
    "    vid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000 45250\n"
     ]
    }
   ],
   "source": [
    "### checking the values of variable.\n",
    "\n",
    "print(maxCommentCount, commentCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to build Function that automates fetching comments\n",
    "Here is the attempt to build the function that makes the youtube search call as well as the comment thread call to build the dataset sequentially and automatically without much interference.\n",
    "\n",
    "But there is a duration window associated the API for a client that could not be handled by this procedure. Hence it was not a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for query in queries:\n",
    "    commentCount = 0\n",
    "    \n",
    "    # Making the request to get video id list.\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults = 100,\n",
    "        q=query,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    print(\"response for query: \", query, \" has been fetched!\")\n",
    "    \n",
    "    # Making list of video ids for iteration.\n",
    "    videosIDS = list()\n",
    "    for i in range(len(response[\"items\"])):\n",
    "        videosIDS.append(response[\"items\"][i][\"id\"][\"videoId\"])\n",
    "    \n",
    "    # open a file for jouralist.\n",
    "    dataFileName = \"\".join(query.split()[:2])+\".csv\"\n",
    "    writeInCSV(headingRow)\n",
    "    \n",
    "    vid = 0\n",
    "    while(vid < len(videosIDS) and commentCount < maxCommentCount):\n",
    "        commentCount += getCommentsFromVideoID(videosIDS[vid])\n",
    "        vid += 1\n",
    "    print(\"Comment Extraction for the query completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the purpose to trying the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emoji.emojize('Python is :thumbs_up:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
